{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "The Bible is the best selling book of all times and continues to be a source for many pointing towards God. Originally written in Hebrew, Aramaic, and Greek, the Bible has been translated into over 600+ languages, with each respective language having its own respective variations and verisons.  \n",
    "\n",
    "In this project, we will be examinign a few English Translations of the Bible and using some natural language procesing (NLP) tools in attempt to see how some of these versions are similar and how they differ.\n",
    "\n",
    "The goal of this project is not to declare which versions are better or worse than another, but simply to see how these versions relate to one another, using Python and some machine learning techniques to quantify and make comparisons.\n",
    "\n",
    "___________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "In order to compare the different Bibles versions, we will first need a source to pull the Bibles from.  For this project, we will utilize the following online APIs to pull Bible text:\n",
    "\n",
    "* Biblia\n",
    "* GetBible\n",
    "\n",
    "Below is a diagram on what this project will look like:\n",
    "\n",
    "![Project_Overview](./assets/resources/images/Capstone_Project_Overview.png \"Overview\")\n",
    "\n",
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de9432884ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes_grid1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageGrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import io\n",
    "import cv2\n",
    "import nltk\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from plotly.subplots import make_subplots\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from keras import models\n",
    "from keras.layers import Dense, Activation\n",
    "from gensim.models import Word2Vec\n",
    "from PIL import Image\n",
    "from API import My_API\n",
    "import text_handling as th\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing APIs for Bible text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store the different versions of the Bible, we will use a dictionary as the main structure to hold the content of each, called `versions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the most care had to be taken into consideration when pulling information from each API.  In an attempt to access the text as easily as possible, I am using a custom API to accesss each site. The queries are inserted and built by hand using the respective documentation at each site.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cannonical books\n",
    "\n",
    "The API _Biblia API_ is a very useful site that is well documented and also has services built in to get the contents fo each specific bible.  To make the text retrieval process more seamless, the list of books within standard canonical bibles are retrieved, then stored for later use within different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Pull the canonical books from Website\n",
    "my_api = My_API(url=\"https://api.biblia.com/v1/bible/\", key='fd37d8f28e95d3be8cb4fbc37e15e18e')\n",
    "query = 'contents/KJV?'\n",
    "resp = my_api.run_query(query)\n",
    "\n",
    "## Strip out the books and remvoe whitespace with '_' \n",
    "# canonical_books = [book['passage'].replace(\" \",\"_\").strip() for book in resp['books']]\n",
    "canonical_books = [book['passage'].strip() for book in resp['books']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(canonical_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions for Bible text retrieval and formatting\n",
    "\n",
    "Text retrieval was the most difficult part of the project. Each API differs significantly and the format of the text returned made this even more difficult. To take care of these differences, custom functions were created and stored in the helper filed, _Text_Handling.py_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format of downloaded text\n",
    "\n",
    "To simplify the retrieval and storage of Bible text across different platforms, the text is stored as fully formatted verse as seen in the example below.\n",
    "\n",
    "```\n",
    "{\"KJV\":\n",
    "   {\n",
    "    \"Genesis 1:1\": \"In the beginning God created heaven, and earth.\",\n",
    "    \"Genesis 1:2\": \"And the earth was void and empty, and darkness was upon the face of the deep; and the spirit of God moved over the waters.\",\n",
    "    \"Genesis 1:3\": \"And God said: Be light made. And light was made.\",\n",
    "    \"Genesis 1:4\": \"And God saw the light that it was good; and he divided the light from the darkness.\",\n",
    "    ...\n",
    "    \"Revelation 22:21\": \"The grace of our Lord Jesus Christ be with you all. Amen.\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Biblia API_\n",
    "\n",
    "This site is very useful and hosts a strong amount of services for pulling Bible text from the website.\n",
    "Finding the right query to use was the most difficiult part of using this query.  \n",
    "\n",
    "As a result, the decision was made to use the following versions below, and offset the remaining versions to another API:\n",
    "\n",
    "Bible Version |\tVersion ID  \n",
    ":---:     |:----:  |\n",
    "1890 Darby Bible |\tDARBY  \n",
    "The Emphasized Bible |\tEMPHBBL \n",
    "King James Version |\tKJV1900  \n",
    "The Lexham English Bible |\tLEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biblia_available_versions = ['KJV1900','LEB','EMPHBBL','DARBY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the content from a specific bible, the `content` service of the API has to be used.  Selecting different services is pretty simple.  To do so, you enter the name of the service as a path, within a directory as seen below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the text for each book from website and store in dictionary for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_api = My_API(url=\"https://api.biblia.com/v1/bible/\", key='fd37d8f28e95d3be8cb4fbc37e15e18e')\n",
    "\n",
    "biblia_versions = {}\n",
    "\n",
    "for v in biblia_available_versions:\n",
    "    print(v)\n",
    "    ver = {}\n",
    "    for book in canonical_books:\n",
    "\n",
    "        query = f'content/{v}.txt?passage={book}&style=oneVersePerLineFullReference'\n",
    "        resp = my_api.run_query(query)\n",
    "\n",
    "        book_dict = th.parse_formatted_verse_ref(resp)\n",
    "        ver.update(book_dict)\n",
    "        \n",
    "    versions[v] = ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _GetBible.net_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This site was the easiest to use and gave the information in the ideal format.\n",
    "Each verse was in its own respective JSON array, requiriing less effort to retrieve the information.\n",
    "\n",
    "Below are the English BIble Versions available at this website:\n",
    "\n",
    "Bible Version |\tVersion ID  \n",
    ":---:     |:----:  \n",
    "American Standard Version |\tASV \n",
    "Authorized Version |\tKJV  \n",
    "The Lexham English Bible |\tLEB \n",
    "Young’s Literal Translation |\tYLT\n",
    "BasicEnglish Bible  | BASICENGLISH\n",
    "Douary Rheims Bible | DOUAYRHEIMS\n",
    "Webster's Bible  | WB\n",
    "World English Bible | WEB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getbible_eng_versions = ['KJV','AKJV','ASV','BASICENGLISH','DOUAYRHEIMS', 'WB','WEB','YLT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_api = My_API(url=\"https://getbible.net/\")\n",
    "\n",
    "for v in getbible_eng_versions:\n",
    "    v_dict = {}\n",
    "    print(v)\n",
    "    for book in canonical_books:\n",
    "        resp = my_api.run_query(f'json?passage={book}&v={v.lower()}')\n",
    "        resp_dict = json.loads(resp[1:-2])\n",
    "        v_dict.update(th.parse_book(resp_dict))\n",
    "    versions[v] = v_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "versions.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are saving the dictionary as a json object to a file, for easier access later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in versions.keys():\n",
    "    key_dict = {key: versions[key]}\n",
    "    with open(f\"./data/{key}.json\", \"w\") as f:\n",
    "        js = json.dump(key_dict, fp=f, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull data from saved file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling files from `/data` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = './data'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for file in onlyfiles:\n",
    "    with open(f\"./data/{file}\") as f:\n",
    "        bible = json.load(f)\n",
    "    data.update(bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will make a dataframe out of the parsed text to get a better idea of how the information is formatted.  All of the versions will be added and general statistics will be calculated to see how the available Bibles differ from one another.\n",
    "\n",
    "Some of these statistics include the following:\n",
    "\n",
    "`char_count` <br>\n",
    "`word_count` <br>\n",
    "`punctuation_count` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes out of the Bible versions stored\n",
    "list_of_df = []\n",
    "for version in data.keys():\n",
    "    content = []\n",
    "    for item in data[version].items():\n",
    "        ref, text = item\n",
    "\n",
    "        book = ' '.join(ref.split()[:-1]).lower().replace(' ','_')\n",
    "        chapter = ref.split()[-1].split(\":\")[0]\n",
    "        verse = ref.split()[-1].split(\":\")[1]\n",
    "        \n",
    "        if book == 'psalm':\n",
    "            book = 'psalms'\n",
    "        if book == 'song_of_solomon':\n",
    "            book = 'song_of_songs'\n",
    "\n",
    "        content.append((version.lower(),book,chapter,verse,text))\n",
    "    \n",
    "    df = pd.DataFrame(data=content, columns=['version','book','chapter','verse','text'])\n",
    "    list_of_df.append(df)\n",
    "\n",
    "df = pd.concat(list_of_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create general word statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['char_count'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['word_density'] = df['char_count'] / (df['word_count']+1)\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "df['title_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "df['upper_case_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.book.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize text and add it as a column in the dataframe\n",
    "# df['tokenize_verses'] = df['text'].apply(lambda x: th.tokenize_verse(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Testament Comparison & New Testament Comparision against different versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggrts = df.groupby(['version']).agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggrts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph function to make bar graph\n",
    "def generate_bar_graph(df=df, metric='', function='', title='Graph'):\n",
    "    fig = go.Figure([go.Bar(x=[item[0]], y=[item[1]], \n",
    "                            name=item[0], showlegend=True) \n",
    "                     for item in df[metric].items()\n",
    "                    ])\n",
    "    fig.update_layout(title_text=title)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate_bar_graph(aggrts, metric='char_count', function='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Graph of metrics sums\n",
    "columns, index = aggrts.columns, 0 \n",
    "rows, cols = 2, len(columns)//2\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(20,15))\n",
    "\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        \n",
    "        barplot = sns.barplot(x='version', y=columns[index], data=aggrts.reset_index(),\n",
    "                    palette=\"muted\", ax=axs[i][j])\n",
    "        barplot.set_xticklabels(barplot.get_xticklabels(), rotation=90)\n",
    "        barplot.set_title(label=f'{columns[index]} by version')\n",
    "        barplot.xaxis.set_label([])\n",
    "        index += 1\n",
    "\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, \n",
    "                    right=0.95, hspace=0.25, wspace=0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggrts = df.groupby(['version']).agg('mean')\n",
    "\n",
    "## Graph of metrics sums\n",
    "columns, index = aggrts.columns, 0 \n",
    "rows, cols = 2, len(columns)//2\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(20,15))\n",
    "\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        \n",
    "        barplot = sns.barplot(x='version', y=columns[index], data=aggrts.reset_index(),\n",
    "                    palette=\"muted\", ax=axs[i][j])\n",
    "        barplot.set_xticklabels(barplot.get_xticklabels(), rotation=90)\n",
    "        barplot.set_title(label=f'{columns[index]} by version')\n",
    "        barplot.xaxis.set_label([])\n",
    "        index += 1\n",
    "\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                    wspace=0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggrts = df.groupby(['version']).agg('median')\n",
    "\n",
    "## Graph of metrics sums\n",
    "columns, index = aggrts.columns, 0 \n",
    "rows, cols = 2, len(columns)//2\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(20,15))\n",
    "\n",
    "for i in range(0,rows):\n",
    "    for j in range(0,cols):\n",
    "        \n",
    "        barplot = sns.barplot(x='version', y=columns[index], data=aggrts.reset_index(),\n",
    "                    palette=\"muted\", ax=axs[i][j])\n",
    "        barplot.set_xticklabels(barplot.get_xticklabels(), rotation=90)\n",
    "        barplot.set_title(label=f'{columns[index]} by version')\n",
    "        barplot.xaxis.set_label([])\n",
    "        index += 1\n",
    "\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                    wspace=0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These are functions taken from Flat Iron Curriculum\n",
    "\n",
    "def count_vectorize(line, vocab=None):\n",
    "\n",
    "    if vocab:  \n",
    "        for word in line:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else: \n",
    "                vocab[word] = 1\n",
    "        return vocab\n",
    "        \n",
    "    else:\n",
    "        unique_words = list(set(line))\n",
    "\n",
    "        text_dict = {i:0 for i in unique_words}\n",
    "        \n",
    "        for word in line:\n",
    "            if word in text_dict:\n",
    "                text_dict[word] += 1\n",
    "            else :\n",
    "                text_dict[word] = 1    \n",
    "        \n",
    "        return text_dict\n",
    "\n",
    "def term_frequency(BoW_dict):\n",
    "    total_word_count = sum(BoW_dict.values())\n",
    "    \n",
    "    for ind, val in BoW_dict.items():\n",
    "        BoW_dict[ind] = val/ total_word_count\n",
    "    \n",
    "    return BoW_dict\n",
    "\n",
    "\n",
    "def inverse_document_frequency(list_of_dicts):\n",
    "    vocab_set = set()\n",
    "    # Iterate through list of dfs and add index to vocab_set\n",
    "    for d in list_of_dicts:\n",
    "        for word in d.keys():\n",
    "            vocab_set.add(word)\n",
    "    \n",
    "    # Once vocab set is complete, create an empty dictionary with a key for each word and value of 0.\n",
    "    full_vocab_dict = {i:0 for i in vocab_set}\n",
    "    \n",
    "    # Loop through each word in full_vocab_dict\n",
    "    for word, val in full_vocab_dict.items():\n",
    "        docs = 0\n",
    "        \n",
    "        # Loop through list of dicts.  Each time a dictionary contains the word, increment docs by 1\n",
    "        for d in list_of_dicts:\n",
    "            if word in d:\n",
    "                docs += 1\n",
    "        \n",
    "        # Now that we know denominator for equation, compute and set IDF value for word\n",
    "        \n",
    "        full_vocab_dict[word] = np.log((len(list_of_dicts)/ float(docs)))\n",
    "    \n",
    "    return full_vocab_dict\n",
    "\n",
    "\n",
    "def tf_idf(list_of_dicts):\n",
    "    \n",
    "    # Create empty dictionary containing full vocabulary of entire corpus\n",
    "    doc_tf_idf = {}\n",
    "    idf = inverse_document_frequency(list_of_dicts)\n",
    "    full_vocab_list = {i:0 for i in list(idf.keys())}\n",
    "    \n",
    "    # Create tf-idf list of dictionaries, containing a dictionary that will be updated for each document\n",
    "    tf_idf_list_of_dicts = []\n",
    "    \n",
    "    # Now, compute tf and then use this to compute and set tf-idf values for each document\n",
    "    for doc in list_of_dicts:\n",
    "        doc_tf = term_frequency(doc)\n",
    "        for word in doc_tf:\n",
    "            doc_tf_idf[word] = doc_tf[word] * idf[word]\n",
    "        tf_idf_list_of_dicts.append(doc_tf_idf)\n",
    "    \n",
    "    return tf_idf_list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_verses = df[(df.version=='asv')].text.apply(lambda x: th.tokenize_verse(x))\n",
    "list_of_verses[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_count = {}\n",
    "for verse in list_of_verses:\n",
    "     vocab_count = count_vectorize(verse, vocab_count)\n",
    "        \n",
    "vocab_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "term_freq = term_frequency(vocab_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inverse_document_frequency([term_freq])\n",
    "tf_idf([term_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data:\n",
    "    list_of_verses = list(data[key].values())\n",
    "    \n",
    "    vocab_count = {}\n",
    "    for verse in list_of_verses:\n",
    "         vocab_count = count_vectorize(verse, vocab_count)\n",
    "    term_freq = term_frequency(vocab_count)\n",
    "    tf_idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-idf vectorizer using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_verses = [' '.join(val) for val in data['KJV'].values()]\n",
    "list_of_verses = df.loc[(df.version=='kjv')].text # .apply(lambda x: th.tokenize_verse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "word_count_vector = cv.fit_transform(list_of_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compute the IDF values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print idf values\n",
    "idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compute the TFIDF score for the document__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count matrix\n",
    "count_vector=cv.transform(list_of_verses)\n",
    " \n",
    "# tf-idf scores\n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    " \n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[0]\n",
    " \n",
    "#print the scores\n",
    "df_vec = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df_vec.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dictionary of models to hold the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions_wv_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    vals = df[(df.version==key.lower())].text.apply(lambda x: th.tokenize_verse(x)).values\n",
    "    model = Word2Vec(vals, size=100, window=5, min_count=1, workers=4)\n",
    "    model.train(vals, total_examples=model.corpus_count, epochs=10)\n",
    "    versions_wv_dict[key] = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_check = ['salvation','resurrection','healing', 'redemption','hope','joy','peace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "for word in words_to_check:\n",
    "    for key in versions_wv_dict.keys():\n",
    "        print(f\"Version: {key} \\t Word: {word}\")\n",
    "        try:\n",
    "            similar_words = versions_wv_dict[key].most_similar(positive=[word], topn=10)\n",
    "            for related in similar_words:\n",
    "                print(related)\n",
    "            print('\\n\\n')\n",
    "        except KeyError:\n",
    "            print(f\"{word} not in the vocabulary\")\n",
    "            print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the least similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the least similar for \n",
    "index = 0\n",
    "for word in words_to_check:\n",
    "    for key in versions_wv_dict.keys():\n",
    "        print(f\"Version: {key} \\t Word: {word}\")\n",
    "        try:\n",
    "            similar_words = versions_wv_dict[key].most_similar(negative=[word], topn=10)\n",
    "            for related in similar_words:\n",
    "                print(related)\n",
    "            print('\\n\\n')\n",
    "        except KeyError:\n",
    "            print(f\"{word} not in the vocabulary\")\n",
    "            print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud Representation\n",
    "\n",
    "In this section, we have code to create a word cloud representation of books within the specified Bible.  This code can be ran on entire versions, to get further insight into similarities between different different versions and books.  I highly recommended going book by book.  Loading entire versions results in long load times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def get_stop_words(file_path=''):\n",
    "    \n",
    "    complete_stoplist = list(STOPWORDS) + list(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    if file_path:\n",
    "        with open(file_path, 'r') as f:\n",
    "            eliz_stopwords = f.readlines()\n",
    "            \n",
    "        eliz_stopwords = [word.strip() for word in eliz_stopwords]\n",
    "        complete_stoplist += eliz_stopwords\n",
    "    \n",
    "    return set(complete_stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(df=None, col='', stopwords=None, save_disp_flag=False, img_width=800, img_height=800):\n",
    "    \"\"\"\n",
    "        Function to generate word cloud. This function generates the cloud and stores it in a file\n",
    "        within the current directory\n",
    "    \"\"\"\n",
    "    comment_words = ' '\n",
    "\n",
    "    for val in df[col]:\n",
    "        tokens = [token for token in th.tokenize_verse(val)]\n",
    "\n",
    "        for words in tokens:\n",
    "            comment_words = comment_words + words + ' '\n",
    "\n",
    "    wordcloud = WordCloud(width = img_width, height = img_height, \n",
    "                    background_color ='white', \n",
    "                    stopwords = stopwords, \n",
    "                    min_font_size = 10).generate(comment_words)\n",
    "    \n",
    "    return wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to create zoomable static image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "def display_wordcloud(width=400, height=200, scl_factor=0.5, img_path='', display_flag=False):\n",
    "    \"\"\"\n",
    "        Original code can be seen at Plotly site.  Modified so that it takes in an image file\n",
    "        For\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Constants\n",
    "    img_width = width\n",
    "    img_height = height\n",
    "    scale_factor = scl_factor\n",
    "\n",
    "    # Add invisible scatter trace.\n",
    "    # This trace is added to help the autoresize logic work.\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, img_width * scale_factor],\n",
    "            y=[0, img_height * scale_factor],\n",
    "            mode=\"markers\",\n",
    "            marker_opacity=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Configure axes\n",
    "    fig.update_xaxes(\n",
    "        visible=False,\n",
    "        range=[0, img_width * scale_factor]\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        visible=False,\n",
    "        range=[0, img_height * scale_factor],\n",
    "        # the scaleanchor attribute ensures that the aspect ratio stays constant\n",
    "        scaleanchor=\"x\"\n",
    "    )\n",
    "\n",
    "    # Add image\n",
    "    fig.add_layout_image(\n",
    "        dict(\n",
    "            x=0,\n",
    "            sizex=img_width * scale_factor,\n",
    "            y=img_height * scale_factor,\n",
    "            sizey=img_height * scale_factor,\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            opacity=1.0,\n",
    "            layer=\"below\",\n",
    "            sizing=\"stretch\",\n",
    "            source=img_path)\n",
    "    )\n",
    "\n",
    "    # Configure other layout\n",
    "    fig.update_layout(\n",
    "        width=img_width * scale_factor,\n",
    "        height=img_height * scale_factor,\n",
    "        margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0},\n",
    "    )\n",
    "    \n",
    "    if display_flag:\n",
    "        # Disable the autosize on double click because it adds unwanted margins around the image\n",
    "        # More detail: https://plot.ly/python/configuration-options/\n",
    "        fig.show(config={'doubleClick': 'reset'})\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, we create the word cloud for the specified verison and book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = get_stop_words(file_path='./assets/resources/custom_stopwords.txt')\n",
    "image_width, image_height = 800, 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncoment the lines below to and change the version & book fields, to generate plot.ly figure for respective version and book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_book = df.loc[((df.version=='asv') & (df.book=='john'))]\n",
    "\n",
    "# image = generate_word_cloud(df_book, col='text', stopwords=stop_list)\n",
    "# fig = display_wordcloud(width=image_width, height=image_height, img_path=image, display_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_book = df.loc[((df.version=='ylt') & (df.book=='john'))]\n",
    "\n",
    "# image = generate_word_cloud(df_book, col='text', stopwords=stop_list)\n",
    "# fig = display_wordcloud(width=image_width, height=image_height, img_path=image, display_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Grid of WordClouds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_wordcloud(df=None, version_list=None, book='genesis', display_flag=False,\n",
    "                            stopword_list=None, grid_size=(33., 33.), image_width=400, \n",
    "                            image_height=400, rows=4, cols=3, axs_pad=0.5):\n",
    "    \n",
    "    fig = plt.figure(figsize=grid_size)\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(rows, cols),  # creates rows x cols grid of axes\n",
    "                     axes_pad=axs_pad,  # pad between axes in inch.\n",
    "                     )\n",
    "    \n",
    "    for ax, version in zip(grid, version_list):\n",
    "        \n",
    "        df_book = df.loc[((df.version==version) & (df.book==book))]\n",
    "\n",
    "        image = generate_word_cloud(df_book, col='text', stopwords=stopword_list, \n",
    "                                    img_height=image_height, img_width=image_width)        \n",
    "        ax.imshow(image)\n",
    "        ax.set_title(version.upper())\n",
    "                \n",
    "    if display_flag:\n",
    "        plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2data(fig):\n",
    "    \"\"\"\n",
    "    @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it\n",
    "    @param fig a matplotlib figure\n",
    "    @return a numpy 3D array of RGBA values\n",
    "    site: http://www.icare.univ-lille1.fr/tutorials/convert_a_matplotlib_figure\n",
    "    \"\"\"\n",
    "    # draw the renderer\n",
    "    fig.canvas.draw()\n",
    " \n",
    "    # Get the RGBA buffer from the figure\n",
    "    w,h = fig.canvas.get_width_height()\n",
    "    buf = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8 )\n",
    "    buf.shape = (w, h, 4)\n",
    " \n",
    "    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode\n",
    "    buf = np.roll(buf, 3, axis=2)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2img(fig):\n",
    "    \"\"\"\n",
    "    @brief Convert a Matplotlib figure to a PIL Image in RGBA format and return it\n",
    "    @param fig a matplotlib figure\n",
    "    @return a Python Imaging Library ( PIL ) image\n",
    "    site: http://www.icare.univ-lille1.fr/tutorials/convert_a_matplotlib_figure\n",
    "    \"\"\"\n",
    "    # put the figure pixmap into a numpy array\n",
    "    buf = fig2data ( fig )\n",
    "    w, h, d = buf.shape\n",
    "    return Image.frombytes( \"RGBA\", ( w ,h ), buf.tostring( ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_from_fig(fig, dpi=180):\n",
    "    \"\"\"\n",
    "        Function that generates high definition image and returns it as a numpy array\n",
    "    \"\"\"\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=dpi)\n",
    "    buf.seek(0)\n",
    "    img_arr = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
    "    buf.close()\n",
    "    img = cv2.imdecode(img_arr, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cloud_grid = generate_multi_wordcloud(df=df, version_list=df.version.unique(), display_flag=True,\n",
    "                                     book='isaiah', stopword_list=stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_img_from_fig(cloud_grid)\n",
    "pillow_image = Image.fromarray(img)\n",
    "pillow_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_wordcloud(img_path=pillow_image, width=1300, height=1300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we found that the versions are similar to one another, with some differences that arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Metrics__ <br>\n",
    "\n",
    "The metrics gave us a superficial means of comparing different versions and seeing how they relate and differ. There seemed to be more similarities overall than differences.  The 4 general statistics used to quantify and visualize the differences were *sum, mean, median,* and _mode._  As we looked at the sums, the character counts were almost identical across all versions of the Bibles compared.  \n",
    "\n",
    "The main metrics that saw the most drastic differences from version to version were **punctuation_count, title_word_count,** and **upper_case_word_count.**  \n",
    "\n",
    "In the graph below, the upper case word count is significantly higher for _AKJV, KJV, KJV1900, and WB_ versions.  Versions of the KJV are noted to be more poetic in interpretation, as well as in the printing of the verses. This same philosophy\n",
    "It is possible that the other versions happen to be lower due to new scripts and advancements in translations that made prior readings more clear and understandable.  This difference in the upper case count also points towards a shift in pronouns and titles used to identify GOD vs those used to identify man.\n",
    "\n",
    "![Average upper_case_word_count_by_verse](./assets/resources/images/average_upper_case_word_count_by_verse.png)\n",
    "\n",
    "\n",
    "To better view the differences in the metrics, I highly recommend running the Dashboard  app.py file, contained within the repository.  Also looking further into the philosophies behind each translation may answer some of the trends visible within the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Word2Vec__ <br>\n",
    "\n",
    "Modeling in Word2Vec was very interesting.  Many of the versions returned at least 2 shared words when we sought to look at words that were most similar.  The results seemed strange once we decided to look at least similar words.  More research would have to be done to better understand what attributed to the difference in results seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Word cloud representation__ <br>\n",
    "\n",
    "I found it interesting comparing the word cloud generations to one another vs the Word2Vec representations.  In the multi-grid wordclouds, many of the same words appeared accross each available version.  The main difference was the font size.  The wordcloud package in use utlizes word frequency to set the font size of each respective word.  In the grid above, we are looking at the book of Isaiah.  All versions contains the words LORD, but it shows up with different sizes.  For other versions such as the WEB, the personal name of GOD is writtten into the translation, in place of the all caps \"LORD.\"  Many of the older versions carry this practice. Based on the results of the word clouds, we can see that there is still a great deal of commonality among the different versions, despite how language has evolved overtime and how translation techniques and paradigms have grown as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the day, a deeper comparison can be made, but it will take much more human intervention to see the qualitive differences.  This remark does not discourage the use of technology to help observe difference in versions. There is still so much more that we can create code to identify.  This is very exciting to me because there is so much more room in understanding Biblical text and developign translations that will better communicate the Bible's central theme to new generations. Data Science can play a very substantial role in bridging the communication gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Works & Additions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add a customizable lematizer to take care of different forms of the same word.\n",
    "  Ex: said, saith, came, come, cometh\n",
    "* Develop functionality for the Dash app that will generate the different word cloud images and pass them through a CNN to find which ones are the most similar\n",
    "* Add functionality that would use the Word2Vec to determine uniqueness vs similarities of the vectors returned.\n",
    "* Make the wordcloud grid available in the Dash app, where each grid would be a subplot that users can interact with\n",
    "* Add similar word functionality to the Dash to allow users to see the output of the Word2Vec models in real-time\n",
    "* Extend the project to automatically compare and contrast other documents\n",
    "* Using a CNN on the different word clouds generated and trainning it to identify versions that are closest, based on the text passed in and|or the word cloud images generated. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
